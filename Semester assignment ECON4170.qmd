---
title: "Semester project for ECON4170"
format: html
editor: visual
execute: 
  echo: false
  warning: false
  message: false
---

## introduction

this is a semester project where we will be making a model to see if we can predict the price of electricity in Norway, one day (24 hours) ahead of time. we want to create a model that beats the naive benchmark.

## Data

the data that we will be using comes from ENTSO-E. we have gotten a yearly report for this year (up to the 27th of October) with the time between measurements being every 15 minutes, across an entire 24-hour period. its an excel file (CSV). the original currency that is displayed in the excel file is in EUR.

we will transform this data to fit our needs as we go. to start, we will be adding a new collumn that converts the currency and price, to something that displays NOK. To do this, we will use todays current exchange rate. (11,68 NOK per EUR, as of the 26th of October 2025)

## Running Code

```{r}

##first dataset: electricity price history

library(tidyverse) #standard package for most projects in this course
library(janitor) #used by professor in seminar 5 solution
library(lubridate) #found in syllabus
library(forecast) #from datacamp course Forecasting in R


ELP <- readr::read_csv("GUI_ENERGY_PRICES_202412312300-202512312300.csv") %>%
  clean_names() %>%
  mutate(
    Price_time = dmy_hms(sub("- .*","",mtu_cet_cest)),
    hourly_time = floor_date(Price_time, "hour"), #setting the intervals to every hour
    Price_EUR = day_ahead_price_eur_m_wh,
    Price_NOK = Price_EUR * 11.68) %>%
  group_by(hourly_time) %>%
  summarise(
    Price_EUR = mean(Price_EUR, na.rm = TRUE),
    Price_NOK = mean(Price_NOK, na.rm = TRUE), 
    .groups = "drop")

## other vaiables: water reservior levels and temprature history

Total_load <- readr::read_csv("GUI_TOTAL_LOAD_DAYAHEAD_202412312300-202512312300.csv") %>%
  clean_names() %>%
  mutate(
    hourly_time = dmy_hm(sub("- .*","",mtu_cet_cest)),
    load = actual_total_load_mw) %>%
  select(hourly_time, load)

# water reservior levels

reservior_Weekly <- readr::read_csv("GUI_WATER_RESERVOIRS_HYDRO_STORAGE_202412312300-202512312300.csv") %>% 
  clean_names() %>%
  separate(time_interval, c("start", "end"), sep = " - .*") %>%
  transmute(
    hourly_time = as.POSIXct(dmy(start)),
    Energy = parse_number(energy_m_wh)) %>%
  arrange(hourly_time)

reservior_hourly <- ELP %>%
  select(hourly_time) %>%
  left_join(reservior_Weekly, by = "hourly_time") %>%
  arrange(hourly_time) %>%
  fill(Energy, .direction = "down") %>%
  fill(Energy, .direction = "up")


ggplot(ELP, aes(x =hourly_time, y = Price_NOK)) + #ggplot to check up on the data
  geom_line() +
  labs(title = "Electricity prices (hourly)")

```

```{r}
#Data Wrangling lufttemperatur#
library(tidyverse)
# reading the data set for temperature in NO1#
temp_NO1 <- read.csv("Lufttemperatur_NO1.csv", sep = ";")

#Removing the station column as its not needed##
temp_NO1$Stasjon<- NULL

#renaming column and time format for merging later##
temp_NO1<- temp_NO1%>%
  rename(time = Tid.norsk.normaltid.)%>%
  mutate(time = as.POSIXct(time, format = "%d.%m.%Y %H:%M"))

#substituting "," for "." in the temprature column to make numereic##
temp_NO1<-temp_NO1%>%
  mutate(Lufttemperatur = gsub(",", ".",Lufttemperatur))

#aking the column numeric#
temp_NO1$Lufttemperatur<- as.numeric(temp_NO1$Lufttemperatur)

#creating avgtem by grouping with time and taking the mean at each timestamp#
AvgTempNO1<-temp_NO1 %>%
group_by(time) %>%
  summarize(AvgTempNO1 = mean(Lufttemperatur, na.rm = TRUE))%>%
  drop_na()

```

```{r}
library(lubridate)
library(tidyverse)
#arranging and fixing to merge the files##
price <- ELP 
supply_water <- reservior_hourly
demand <- Total_load
temperature<- AvgTempNO1

#renaming and removing coloumns in supply, price and demand##
supply_water<-supply_water %>%
  rename(time = hourly_time)

price<-price %>%
  rename(time = hourly_time) %>%
  select(time, Price_NOK)

demand<- demand %>%
  rename(time = hourly_time, demand = load) %>%
  mutate(demand = as.numeric(demand))

#need to remove first row from supply, water and demand to fit the time window of temperature#
supply_water<-supply_water[-1,]
demand<- demand [-1,]
price<-price[-1,]

#merging the data#
model_data <- temperature %>%
  inner_join(price,by = "time") %>%
  inner_join(supply_water, by = "time") %>%
  inner_join(demand, by = "time")

#convert timezpne to oslo##

model_data<- model_data %>%
  mutate(time = with_tz(time, "CET"))

#set end timestamp of data to 25.10.25 23:00 because of timechange on the 26th makes data not aligned so it saves trouble ending a few days earlier

library(dplyr)

model_data <- model_data %>% 
  filter(time <= "2025-10-25 23:00:00")
```


```{r}
model_data_winsorized <- model_data %>%
  mutate(
    across(where(is.numeric), ~ {
    low <- quantile(.x, 0.01, na.rm = TRUE)
    high <- quantile(.x, 0.99, na.rm = TRUE)
    pmin(pmax(.x, low), high)}))


ggplot(model_data_winsorized, aes(x = time, y = Price_NOK)) +
  geom_line(size = 0.45)
ggplot(model_data_winsorized, aes(x = time, y = AvgTempNO1)) +
  geom_line(size = 0.45)
ggplot(model_data_winsorized, aes(x = time, y = Energy)) +
  geom_line()
ggplot(model_data_winsorized, aes(x = time, y = demand)) +
  geom_line(size = 0.1) #graphs tell us that there is a trend with increasing temp an decreasing demand for energy.


cor(model_data_winsorized$Price_NOK, model_data_winsorized$AvgTempNO1) #(-0.28) higher temp means lower price.
cor(model_data_winsorized$demand, model_data_winsorized$AvgTempNO1) # (-0.856) lower temps means higher demand for energy (heating)


#because the higher the tempreature is, the lower demand is, we can find a point in temp that makes for a better variable to work with. saying that a temp of 18 degrees and above makes for a demand of 0 on heating. making temp a more linear variable the lower temp drops. easier to model with.
#also creating lags for the variables we have

clean_data <- model_data_winsorized %>%
  arrange(time) %>%
  mutate(HDH = pmax(0, 18 - AvgTempNO1)) %>% 
  mutate(
    price_lag1 = lag(Price_NOK, 1),
    price_lag2 = lag(Price_NOK, 2),
    price_lag24 = lag(Price_NOK, 24),
    price_lag168 = lag(Price_NOK, 168),
    
    HDH = HDH,
    HDH_lag24 = lag(HDH, 24),
    
    demand = demand,
    demand_lag24 = lag(demand, 24),
    
    Energy = Energy,
    supply_lag168 = lag(Energy, 168))

#testing for correlations again: 
cor(clean_data$Price_NOK, clean_data$HDH) #higher HDH means a higher price
cor(clean_data$demand, clean_data$HDH) #higher HDH increases demand for energy

clean_data_dummies <- clean_data %>%
  mutate(
    hour = as.integer(format(time, "%H")),
    weekday = as.integer(format(time, "%u")),
    month = as.integer(format(time, "%m")),
    dummy_hour = factor(hour),
    dummy_weekday = factor(weekday),
    dummy_month = factor(month))
```

```{r}
# LAG selection rolling cross-validation with help of AI
#  Step 1 - Candidate lag sets ----
price_lags_list     <- list(c(1), c(1, 24), c(1, 24, 168), c(1, 2, 24, 168))
HDH_lags_list       <- list(c(0), c(0, 24))
load_lags_list      <- list(c(0), c(0, 24))       # demand
reservoir_lags_list <- list(c(0), c(0, 168))      # Energy (reservoir)

# Optional: basic check
stopifnot(all(c("time", "Price_NOK", "demand", "Energy") %in% names(clean_data)))

#  Step 2 - Rolling expanding-window CV ----
library(dplyr)

rolling_cv_rmse <- function(df, price_lags, hdh_lags, load_lags, reservoir_lags,
                            min_train = 24 * 14) {
  # Purpose:
  #  - Expanding window CV
  #  - Forecast 1-step ahead each time using linear ADL (lm)
  #  - Return mean RMSE across all steps

  df_lag <- df %>%
  mutate(
    # Create lagged features dynamically (explicitly reference df$)
    !!!setNames(lapply(price_lags, \(L) dplyr::lag(df$Price_NOK, L)),
                paste0("price_lag", price_lags)),
    !!!setNames(lapply(hdh_lags, \(L) dplyr::lag(df$HDH, L)),
                paste0("HDH_lag", hdh_lags)),
    !!!setNames(lapply(load_lags, \(L) dplyr::lag(df$demand, L)),
                paste0("demand_lag", load_lags)),
    !!!setNames(lapply(reservoir_lags, \(L) dplyr::lag(df$Energy, L)),
                paste0("supply_lag", reservoir_lags))
  ) %>%
  drop_na()


  # Safety: if not enough obs for training + test, return Inf
  if (nrow(df_lag) < (min_train + 1)) return(Inf)

  # Build formula automatically
  preds <- c(
    paste0("price_lag", price_lags),
    paste0("HDH_lag", hdh_lags),
    paste0("demand_lag", load_lags),
    paste0("supply_lag", reservoir_lags)
  )
  fml <- as.formula(paste("Price_NOK ~", paste(preds, collapse = " + ")))

  y <- df_lag$Price_NOK
  n <- nrow(df_lag)
  errors <- numeric(0)

  # Expanding window loop
 for (t in seq(min_train + 1, n, by = 4)) {   # evaluate every 4th hour (~6x faster)

    train_idx <- 1:(t - 1)
    test_idx  <- t

    fit <- try(lm(fml, data = df_lag[train_idx, , drop = FALSE]), silent = TRUE)
    if (inherits(fit, "try-error")) return(Inf)

    pred <- predict(fit, newdata = df_lag[test_idx, , drop = FALSE])
    errors <- c(errors, y[test_idx] - pred)
  }

  sqrt(mean(errors^2))
}
#  Step 3 - Evaluate all lag combinations ----
library(purrr)
library(tibble)

# Build all combinations of lag-set indices
grid <- expand.grid(
  price = seq_along(price_lags_list),
  HDH = seq_along(HDH_lags_list),
  load = seq_along(load_lags_list),
  reservoir = seq_along(reservoir_lags_list),
  KEEP.OUT.ATTRS = FALSE
)

# Run rolling CV for each combination
results <- pmap_dfr(
  as.list(data.frame(grid)),
  function(price, HDH, load, reservoir) {
    pl <- price_lags_list[[price]]
    hl <- HDH_lags_list[[HDH]]
    ll <- load_lags_list[[load]]
    rl <- reservoir_lags_list[[reservoir]]

    rmse <- rolling_cv_rmse(
      df = clean_data,
      price_lags = pl,
      hdh_lags = hl,
      load_lags = ll,
      reservoir_lags = rl,
      min_train = 24 * 14
    )

    tibble(
      price, HDH, load, reservoir,
      RMSE = rmse,
      complexity = length(pl) + length(hl) + length(ll) + length(rl)
    )
  }
) %>%
  arrange(RMSE, complexity)

# Preview results
print(results)

#  Step 4 - Parsimony rule ----

# Find the best RMSE overall
best_rmse <- min(results$RMSE, na.rm = TRUE)

# Keep all models within 1% of best
final_pick <- results %>%
  filter(RMSE <= best_rmse * 1.01) %>%          # within 1%
  arrange(complexity, RMSE) %>%                 # simplest first
  slice(1)                                      # pick top

# Extract the actual lag configuration
chosen_price_lags     <- price_lags_list[[final_pick$price]]
chosen_HDH_lags       <- HDH_lags_list[[final_pick$HDH]]
chosen_load_lags      <- load_lags_list[[final_pick$load]]
chosen_reservoir_lags <- reservoir_lags_list[[final_pick$reservoir]]

# Display selection
print(final_pick)
print(list(
  price_lags     = chosen_price_lags,
  HDH_lags       = chosen_HDH_lags,
  load_lags      = chosen_load_lags,
  reservoir_lags = chosen_reservoir_lags
))
#Step 5 - Save chosen lag configuration ----
final_lags <- list(
  price_lags     = chosen_price_lags,
  HDH_lags       = chosen_HDH_lags,
  load_lags      = chosen_load_lags,
  reservoir_lags = chosen_reservoir_lags
)

save(final_lags, file = "chosen_lags.RData")

# 
```




```{r}
#Benchmark model##

benchmarkmodel <- clean_data%>%
  select(Price_NOK, price_lag1, price_lag24, price_lag168)%>%
  drop_na() %>% 
  mutate(error_naive_1 = Price_NOK - price_lag1,
         error_naive_24 = Price_NOK - price_lag24,
         error_naive_168 = Price_NOK - price_lag168)

#split the data to only 80% to compare with later model##

split <- round(nrow(benchmarkmodel) * 0.80)

#creating the test set##
test <- benchmarkmodel[(split + 1):nrow(benchmarkmodel), ]

#metrics for the benchmark model##

metrics_lag_1 <- accuracy(test$price_lag1, test$Price_NOK)
metrics_lag_24 <- accuracy(test$price_lag24, test$Price_NOK)
metrics_lag_168 <- accuracy(test$price_lag168, test$Price_NOK)
```


```{r}
#creating the adl model

final_data <- clean_data_dummies %>%
  mutate(y_next = lead(Price_NOK, 24)) %>%
  drop_na()

#saving the formula for later use#
struktur <- y_next ~ price_lag1 + price_lag2 + price_lag24 + price_lag168 + HDH + demand + Energy + dummy_hour + dummy_weekday + dummy_month

model <- lm(struktur, data = final_data)

summary(model)

#acf plot to test for white noise#
acf(residuals(model), lag.max = 168)
```
```{r}
###trying with sarimax because strong autocorrelation in adl model###

final_data <- clean_data_dummies %>%
  mutate(y_next = lead(Price_NOK, 24)) %>%
  drop_na()

#using struktur from earlier but dopping price as arima handles price dynamics#

struktur_xreg <- y_next ~ HDH + demand + Energy + dummy_hour + dummy_weekday + dummy_month - 1

#build y and x for the sarimax model#

y<- ts(final_data$y_next, frequency = 24)

#we also use this from the previous code but with some modfications#
x <- model.matrix(struktur_xreg, data = final_data)

# arima model has its own constant and the dummy_hour creates it's own constant aswell so there are two constants meaning duplicate info. By removing hour 0 the model looks at the other by how much higher or lower they are compared to hour 0 #

x <- x[, -match(grep("^dummy_hour",   colnames(x), value = TRUE)[1], colnames(x)), drop = FALSE]
x <- x[, -match(grep("^dummy_weekday", colnames(x), value = TRUE)[1], colnames(x)), drop = FALSE]
x <- x[, -match(grep("^dummy_month",   colnames(x), value = TRUE)[1], colnames(x)), drop = FALSE]


##building the sarimax and setting max p and q for faster runs#

sarimax <- auto.arima(y, xreg = x, seasonal = TRUE , stepwise = TRUE, approximation = TRUE, max.p = 3, max.q = 3, max.P = 1, max.Q = 1)

#summary from previous just switching out model#

summary(sarimax)

#acf with new model#

acf(residuals(sarimax), lag.max = 168)

checkresiduals(sarimax)

```

```{r}
## From previous code assigned to new model##
x_new <- model.matrix(struktur_xreg, data = final_data)

### dropping values as done earlier ###
x_new <- x_new [, -match(grep("^dummy_hour",   colnames(x_new), value = TRUE)[1], colnames(x_new)), drop = FALSE]
x_new <- x_new[, -match(grep("^dummy_weekday", colnames(x_new), value = TRUE)[1], colnames(x_new)), drop = FALSE]
x_new <- x_new[, -match(grep("^dummy_month",   colnames(x_new), value = TRUE)[1], colnames(x_new)), drop = FALSE]


keep_cols <- colnames(x_new)
length(keep_cols)
head(keep_cols)

##creating train and test splits#

# Determine row to split
train_set <- floor(nrow(final_data) * 0.80)

# Create train
train <- final_data[1:train_set, ]

# Create test
test <- final_data[(train_set + 1):nrow(final_data), ]


```








```{r}
#CHecks 


#simple model

###trying with sarimax because strong autocorrelation in adl model###

final_data <- clean_data_dummies %>%
  mutate(y_next = lead(Price_NOK, 24)) %>%
  drop_na()

#build y and x for the sarimax model#

y<- ts(final_data$y_next, frequency = 24)

struktur_xreg <- y_next ~ HDH + demand + dummy_hour - 1
x <- model.matrix(struktur_xreg, data = final_data)
x <- x[, colnames(x) != "dummy_hour0", drop = FALSE]
sarimax_simpler <- auto.arima(y, xreg = x, seasonal = TRUE, stepwise = TRUE, approximation = TRUE,
                              max.p = 3, max.q = 3, max.P = 1, max.Q = 1)
summary(sarimax_simpler)

#acf with new model#

acf(residuals(sarimax), lag.max = 168)

checkresiduals(sarimax)

```




