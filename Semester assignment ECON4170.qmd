---
title: "Semester project for ECON4170"
format: html
editor: visual
execute: 
  echo: false
  warning: false
  message: false
---

## introduction

this is a semester project where we will be making a model to see if we can predict the price of electricity in Norway, one day (24 hours) ahead of time. we want to create a model that beats the naive benchmark.

## Data

the data that we will be using comes from ENTSO-E. we have gotten a yearly report for this year (up to the 27th of October) with the time between measurements being every 15 minutes, across an entire 24-hour period. its an excel file (CSV). the original currency that is displayed in the excel file is in EUR.

we will transform this data to fit our needs as we go. to start, we will be adding a new collumn that converts the currency and price, to something that displays NOK. To do this, we will use todays current exchange rate. (11,68 NOK per EUR, as of the 26th of October 2025)

## Running Code

```{r}

##first dataset: electricity price history

library(tidyverse) #standard package for most projects in this course
library(janitor) #used by professor in seminar 5 solution
library(lubridate) #found in syllabus
library(forecast) #from datacamp course Forecasting in R
install.packages("gridExtra")# To make tables in in png

#Creating folders to save output in later
dir.create("output", showWarnings = FALSE)
dir.create("output/data", showWarnings = FALSE)
dir.create("output/figures", showWarnings = FALSE)
dir.create("output/tables", showWarnings = FALSE)

ELP <- readr::read_csv("GUI_ENERGY_PRICES_202412312300-202512312300.csv") %>%
  clean_names() %>%
  mutate(
    Price_time = dmy_hms(sub("- .*","",mtu_cet_cest)),
    hourly_time = floor_date(Price_time, "hour"), #setting the intervals to every hour
    Price_EUR = day_ahead_price_eur_m_wh,
    Price_NOK = Price_EUR * 11.68) %>%
  group_by(hourly_time) %>%
  summarise(
    Price_EUR = mean(Price_EUR, na.rm = TRUE),
    Price_NOK = mean(Price_NOK, na.rm = TRUE), 
    .groups = "drop")

## other vaiables: water reservior levels and temprature history

Total_load <- readr::read_csv("GUI_TOTAL_LOAD_DAYAHEAD_202412312300-202512312300.csv") %>%
  clean_names() %>%
  mutate(
    hourly_time = dmy_hm(sub("- .*","",mtu_cet_cest)),
    load = actual_total_load_mw) %>%
  select(hourly_time, load)

# water reservior levels

reservior_Weekly <- readr::read_csv("GUI_WATER_RESERVOIRS_HYDRO_STORAGE_202412312300-202512312300.csv") %>% 
  clean_names() %>%
  separate(time_interval, c("start", "end"), sep = " - .*") %>%
  transmute(
    hourly_time = as.POSIXct(dmy(start)),
    Energy = parse_number(energy_m_wh)) %>%
  arrange(hourly_time)

reservior_hourly <- ELP %>%
  select(hourly_time) %>%
  left_join(reservior_Weekly, by = "hourly_time") %>%
  arrange(hourly_time) %>%
  fill(Energy, .direction = "down") %>%
  fill(Energy, .direction = "up")

ELP_plot <- ggplot(ELP, aes(x =hourly_time, y = Price_NOK)) + #ggplot to check up on the data
  geom_line() +
  labs(title = "Electricity prices (hourly)")
ggsave(filename = "output/figures/ELP.png", plot = ELP_plot)

```

```{r}
#Data Wrangling lufttemperatur#
library(tidyverse)
# reading the data set for temperature in NO1#
temp_NO1 <- read.csv("Lufttemperatur_NO1.csv", sep = ";")

#Removing the station column as its not needed##
temp_NO1$Stasjon<- NULL

#renaming column and time format for merging later##
temp_NO1<- temp_NO1%>%
  rename(time = Tid.norsk.normaltid.)%>%
  mutate(time = as.POSIXct(time, format = "%d.%m.%Y %H:%M"))

#substituting "," for "." in the temprature column to make numereic##
temp_NO1<-temp_NO1%>%
  mutate(Lufttemperatur = gsub(",", ".",Lufttemperatur))

#aking the column numeric#
temp_NO1$Lufttemperatur<- as.numeric(temp_NO1$Lufttemperatur)

#creating avgtem by grouping with time and taking the mean at each timestamp#
AvgTempNO1<-temp_NO1 %>%
group_by(time) %>%
  summarize(AvgTempNO1 = mean(Lufttemperatur, na.rm = TRUE))%>%
  drop_na()

```

```{r}
library(lubridate)
library(tidyverse)
#arranging and fixing to merge the files##
price <- ELP 
supply_water <- reservior_hourly
demand <- Total_load
temperature<- AvgTempNO1

#renaming and removing coloumns in supply, price and demand##
supply_water<-supply_water %>%
  rename(time = hourly_time)

price<-price %>%
  rename(time = hourly_time) %>%
  select(time, Price_NOK)

demand<- demand %>%
  rename(time = hourly_time, demand = load) %>%
  mutate(demand = as.numeric(demand))

#need to remove first row from supply, water and demand to fit the time window of temperature#
supply_water<-supply_water[-1,]
demand<- demand [-1,]
price<-price[-1,]

#merging the data#
model_data <- temperature %>%
  inner_join(price,by = "time") %>%
  inner_join(supply_water, by = "time") %>%
  inner_join(demand, by = "time")

#convert timezpne to oslo##

model_data<- model_data %>%
  mutate(time = with_tz(time, "CET"))

#set end timestamp of data to 25.10.25 23:00 because of timechange on the 26th makes data not aligned so it saves trouble ending a few days earlier

library(dplyr)

model_data <- model_data %>% 
  filter(time <= "2025-10-25 23:00:00")
#Saving data
write.csv(model_data,"output/data/model_data.csv")
```

```{r}
model_data_winsorized <- model_data %>%
  mutate(
    across(where(is.numeric), ~ {
    low <- quantile(.x, 0.01, na.rm = TRUE)
    high <- quantile(.x, 0.99, na.rm = TRUE)
    pmin(pmax(.x, low), high)}))

#plotting data

price_plot<-ggplot(model_data_winsorized, aes(x = time, y = Price_NOK)) +
  geom_line(size = 0.45)
avgtemp_plot<-ggplot(model_data_winsorized, aes(x = time, y = AvgTempNO1)) +
  geom_line(size = 0.45)
Energy_plot<-ggplot(model_data_winsorized, aes(x = time, y = Energy)) +
  geom_line()
demand_plot<-ggplot(model_data_winsorized, aes(x = time, y = demand)) +
  geom_line(size = 0.1) #graphs tell us that there is a trend with increasing temp an decreasing demand for energy.
 #saving plots 
ggsave(filename = "output/figures/price.png", plot = price_plot)
ggsave(filename = "output/figures/avgtemp.png", plot = avgtemp_plot)
ggsave(filename = "output/figures/Energy.png", plot = Energy_plot)
ggsave(filename = "output/figures/demand.png", plot = demand_plot)


cor(model_data_winsorized$Price_NOK, model_data_winsorized$AvgTempNO1) #(-0.28) higher temp means lower price.
cor(model_data_winsorized$demand, model_data_winsorized$AvgTempNO1) # (-0.856) lower temps means higher demand for energy (heating)


#because the higher the tempreature is, the lower demand is, we can find a point in temp that makes for a better variable to work with. saying that a temp of 18 degrees and above makes for a demand of 0 on heating. making temp a more linear variable the lower temp drops. easier to model with.
#also creating lags for the variables we have

clean_data <- model_data_winsorized %>%
  arrange(time) %>%
  mutate(HDH = pmax(0, 18 - AvgTempNO1)) %>% 
  mutate(
    price_lag1 = lag(Price_NOK, 1),
    price_lag2 = lag(Price_NOK, 2),
    price_lag24 = lag(Price_NOK, 24),
    price_lag168 = lag(Price_NOK, 168),
    
    HDH = HDH,
    HDH_lag24 = lag(HDH, 24),
    
    demand = demand,
    demand_lag24 = lag(demand, 24),
    
    Energy = Energy,
    supply_lag168 = lag(Energy, 168))

#testing for correlations again: 
cor(clean_data$Price_NOK, clean_data$HDH) #higher HDH means a higher price
cor(clean_data$demand, clean_data$HDH) #higher HDH increases demand for energy

clean_data_dummies <- clean_data %>%
  mutate(
    hour = as.integer(format(time, "%H")),
    weekday = as.integer(format(time, "%u")),
    month = as.integer(format(time, "%m")),
    dummy_hour = factor(hour),
    dummy_weekday = factor(weekday),
    dummy_month = factor(month))
#this drops all unused values##
clean_data_dummies <- clean_data_dummies %>%
  mutate(
    dummy_hour = droplevels(dummy_hour),
    dummy_weekday = droplevels(dummy_weekday),
    dummy_month = droplevels(dummy_month)
  )

#saving the clean_data_dummies
write.csv(clean_data_dummies,"output/data/clean_data_dummies.csv")
```

```{r}
############################AI chunk#########################
###################################################################
##################################################################
#LAG selection rolling cross-validation with help of AI
#####################################################################
#################################################################
#  Step 1 - Candidate lag sets ----
price_lags_list     <- list(c(1), c(1, 24), c(1, 24, 168), c(1, 2, 24, 168))
HDH_lags_list       <- list(c(0), c(0, 24))
load_lags_list      <- list(c(0), c(0, 24))       # demand
reservoir_lags_list <- list(c(0), c(0, 168))      # Energy (reservoir)

# Optional: basic check
stopifnot(all(c("time", "Price_NOK", "demand", "Energy") %in% names(clean_data)))

#  Step 2 - Rolling expanding-window CV ----
library(dplyr)

rolling_cv_rmse <- function(df, price_lags, hdh_lags, load_lags, reservoir_lags,
                            min_train = 24 * 14) {
  # Purpose:
  #  - Expanding window CV
  #  - Forecast 1-step ahead each time using linear ADL (lm)
  #  - Return mean RMSE across all steps

  df_lag <- df %>%
  mutate(
    # Create lagged features dynamically (explicitly reference df$)
    !!!setNames(lapply(price_lags, \(L) dplyr::lag(df$Price_NOK, L)),
                paste0("price_lag", price_lags)),
    !!!setNames(lapply(hdh_lags, \(L) dplyr::lag(df$HDH, L)),
                paste0("HDH_lag", hdh_lags)),
    !!!setNames(lapply(load_lags, \(L) dplyr::lag(df$demand, L)),
                paste0("demand_lag", load_lags)),
    !!!setNames(lapply(reservoir_lags, \(L) dplyr::lag(df$Energy, L)),
                paste0("supply_lag", reservoir_lags))
  ) %>%
  drop_na()


  # Safety: if not enough obs for training + test, return Inf
  if (nrow(df_lag) < (min_train + 1)) return(Inf)

  # Build formula automatically
  preds <- c(
    paste0("price_lag", price_lags),
    paste0("HDH_lag", hdh_lags),
    paste0("demand_lag", load_lags),
    paste0("supply_lag", reservoir_lags)
  )
  fml <- as.formula(paste("Price_NOK ~", paste(preds, collapse = " + ")))

  y <- df_lag$Price_NOK
  n <- nrow(df_lag)
  errors <- numeric(0)

  # Expanding window loop
 for (t in seq(min_train + 1, n, by = 4)) {   # evaluate every 4th hour (~6x faster)

    train_idx <- 1:(t - 1)
    test_idx  <- t

    fit <- try(lm(fml, data = df_lag[train_idx, , drop = FALSE]), silent = TRUE)
    if (inherits(fit, "try-error")) return(Inf)

    pred <- predict(fit, newdata = df_lag[test_idx, , drop = FALSE])
    errors <- c(errors, y[test_idx] - pred)
  }

  sqrt(mean(errors^2))
}
#  Step 3 - Evaluate all lag combinations ----
library(purrr)
library(tibble)

# Build all combinations of lag-set indices
grid <- expand.grid(
  price = seq_along(price_lags_list),
  HDH = seq_along(HDH_lags_list),
  load = seq_along(load_lags_list),
  reservoir = seq_along(reservoir_lags_list),
  KEEP.OUT.ATTRS = FALSE
)

# Run rolling CV for each combination
results <- pmap_dfr(
  as.list(data.frame(grid)),
  function(price, HDH, load, reservoir) {
    pl <- price_lags_list[[price]]
    hl <- HDH_lags_list[[HDH]]
    ll <- load_lags_list[[load]]
    rl <- reservoir_lags_list[[reservoir]]

    rmse <- rolling_cv_rmse(
      df = clean_data,
      price_lags = pl,
      hdh_lags = hl,
      load_lags = ll,
      reservoir_lags = rl,
      min_train = 24 * 14
    )

    tibble(
      price, HDH, load, reservoir,
      RMSE = rmse,
      complexity = length(pl) + length(hl) + length(ll) + length(rl)
    )
  }
) %>%
  arrange(RMSE, complexity)

# Preview results
print(results)

#  Step 4 - Parsimony rule ----

# Find the best RMSE overall
best_rmse <- min(results$RMSE, na.rm = TRUE)

# Keep all models within 1% of best
final_pick <- results %>%
  filter(RMSE <= best_rmse * 1.01) %>%          # within 1%
  arrange(complexity, RMSE) %>%                 # simplest first
  slice(1)                                      # pick top

# Extract the actual lag configuration
chosen_price_lags     <- price_lags_list[[final_pick$price]]
chosen_HDH_lags       <- HDH_lags_list[[final_pick$HDH]]
chosen_load_lags      <- load_lags_list[[final_pick$load]]
chosen_reservoir_lags <- reservoir_lags_list[[final_pick$reservoir]]

# Display selection
print(final_pick)
print(list(
  price_lags     = chosen_price_lags,
  HDH_lags       = chosen_HDH_lags,
  load_lags      = chosen_load_lags,
  reservoir_lags = chosen_reservoir_lags
))
#Step 5 - Save chosen lag configuration ----
final_lags <- list(
  price_lags     = chosen_price_lags,
  HDH_lags       = chosen_HDH_lags,
  load_lags      = chosen_load_lags,
  reservoir_lags = chosen_reservoir_lags
)
# load the existing file
load("chosen_lags.RData")   # this brings final_lags into the environment

save(final_lags, file = "output/data/chosen_lags.RData")

##################################################################################################################################################
#End of AI generated Code
####################################################################################################################
```

```{r}
#creating the adl model

final_data <- clean_data_dummies %>%
  mutate(y_next = lead(Price_NOK, 24)) %>%
  drop_na()

#saving the formula for later use#
struktur <- y_next ~ price_lag1 + price_lag2 + price_lag24 + price_lag168 + HDH + demand + Energy + dummy_hour + dummy_weekday + dummy_month

adl_model <- lm(struktur, data = final_data)

summary(adl_model)

#acf plot to test for white noise#
acf(residuals(adl_model), lag.max = 168)

#saving the acf plot
# Source - https://stackoverflow.com/a
# Posted by Andrie, modified by community. See post 'Timeline' for change history
# Retrieved 2025-11-17, License - CC BY-SA 3.0

png(filename="output/figures/acf_adl_model.png")
plot(acf(residuals(adl_model), lag.max = 168))
dev.off()


#saving the summary
# Source - https://stackoverflow.com/a
# Posted by Phil, modified by community. See post 'Timeline' for change history
# Retrieved 2025-11-17, License - CC BY-SA 4.0

sink("output/tables/adl_model.txt")
print(summary(adl_model))
sink()  # returns output to the console

```

```{r}
###trying with sarimax because strong autocorrelation in adl model###

final_data <- clean_data_dummies %>%
  mutate(y_next = lead(Price_NOK, 24)) %>%
  drop_na()

#using struktur from earlier but dopping price as arima handles price dynamics#

struktur_xreg <- y_next ~ price_lag24 + HDH + demand + Energy + dummy_hour + dummy_weekday + dummy_month

#build y and x for the sarimax model#

y<- ts(final_data$y_next, frequency = 24)

#we also use this from the previous code but with some modfications#
x <- model.matrix(struktur_xreg, data = final_data)

sarimax <- forecast::Arima(
  y, 
  order = c(1,0,1), 
  seasonal = list(order = c(1,0,1), period = 24),
  xreg = x, include.mean = FALSE
)
forecast::checkresiduals(sarimax)

#summary from previous just switching out model#

summary(sarimax)

#acf with new model#

acf(residuals(sarimax), lag.max = 168)

#saving the acf plot
# Source - https://stackoverflow.com/a
# Posted by Andrie, modified by community. See post 'Timeline' for change history
# Retrieved 2025-11-17, License - CC BY-SA 3.0

png(filename="output/figures/sarimax.png")
plot(acf(residuals(sarimax), lag.max = 168))
dev.off()


#saving the summary
# Source - https://stackoverflow.com/a
# Posted by Phil, modified by community. See post 'Timeline' for change history
# Retrieved 2025-11-17, License - CC BY-SA 4.0

sink("output/tables/sarimax.txt")
print(summary(sarimax))
sink()  # returns output to the console

```

\

```{r}

##creating train and test splits#

# Determine row to split
train_set <- floor(nrow(final_data) * 0.80)

# Create train
#train <- final_data[1:train_set, ]

# Create test
test <- final_data[(train_set + 1):nrow(final_data), ]

#a place to store the forecasts and accuracy metrics #AI##

rolling_forecasts <- numeric(nrow(test))
actuals <- test$y_next

## creating the loop with help of AI and reusing previous data##
for (i in seq_len(nrow(test))) {
  ################AI#####################
  if (i %% 10 == 0) cat("Iteration:", i, "of", nrow(test), "at", format(Sys.time(), "%H:%M:%S"), "\n")
############END OF AI GENERATED CODE######################
  train_slice <- final_data[1:(train_set + i - 1), ]
  
  #Y arima training
  y_train <- ts(train_slice$y_next, frequency = 24)
  
  #x arima training 
  x_train <- model.matrix(struktur_xreg, data = train_slice)

#####################################AI##############################
# Compute basic stats (variance, min, max) for every column in x_train.
# This tells us if a dummy variable is "dead" (all values = 0).
stats <- apply(x_train, 2, function(z) c(var = var(z), min = min(z), max = max(z)))

# Identify columns that are ALWAYS zero.
# A column is all-zero if:
#   - variance = 0  (no variation)
#   - min = 0       (lowest value is 0)
#   - max = 0       (highest value is 0)
drop_cols <- names(which(stats["var", ] == 0 &
                         stats["min", ] == 0 &
                         stats["max", ] == 0))

# If such columns exist, remove them from x_train.
# This prevents ARIMA from crashing due to rank-deficient regressors.
if (length(drop_cols) > 0) {
  x_train <- x_train[, !(colnames(x_train) %in% drop_cols), drop = FALSE]
}
  
#################################END OF AI genereated code ######################

## just copy and paste our previous model but change the y and x
  
  temp_model <-  forecast::Arima(
  y_train,
  order = c(1,0,1), 
  seasonal = list(order = c(1,0,1), period = 24),
  xreg = x_train , include.mean = FALSE,
  method = "CSS" #faster runtime <0.5%rmse impact
) 
if (i == 1) cat("method used:", temp_model$method, "\n")

  test_row <- test[i, , drop = FALSE]

  # Only need to create the x_test model matrix for the test## copy and paste previous code with modifications(switching the name and Data to test_model)
  
  x_test <- model.matrix(struktur_xreg, data = test_row)
  
#################AI#####################
miss <- setdiff(colnames(x_train), colnames(x_test))
if (length(miss) > 0) for (m in miss) x_test <- cbind(x_test, setNames(list(0), m))
x_test <- x_test[, colnames(x_train), drop = FALSE]
##############END OF AI###########################
# forecast using the model
fc <- forecast::forecast(temp_model, xreg = x_test, h = 1)

  # store
  rolling_forecasts[i] <- as.numeric(fc$mean)
}
# creating error and calculating rmse and mae

errors<- actuals - rolling_forecasts
RMSE_sarimax <- sqrt(mean(errors^2))
MAE_sarimax <- mean(abs(errors))

##saving output#
saveRDS(
  list(
    rolling_forecasts = rolling_forecasts,
    actuals           = actuals,
    errors            = errors,
    RMSE_sarimax      = RMSE_sarimax,
    MAE_sarimax       = MAE_sarimax
  ),
  file = "sarimax_test_results.rds"
)

```

```{r}

sarimax_results <- readRDS("sarimax_test_results.rds")

saveRDS(sarimax_results, file = "output/data/sarimax_results.rds")

#Benchmark model##

benchmarkmodel <- final_data%>%
  select(time, Price_NOK, price_lag1, price_lag24, price_lag168)%>%
  mutate(error_naive_1 = Price_NOK - price_lag1,
         error_naive_24 = Price_NOK - price_lag24,
         error_naive_168 = Price_NOK - price_lag168)

#creating the test set from sarimax test_set to align 
test_bench <- benchmarkmodel[(train_set + 1):nrow(benchmarkmodel), ]

#metrics for the benchmark model##

metrics_lag_1 <- accuracy(test_bench$Price_NOK, test_bench$price_lag1)
metrics_lag_24 <- accuracy(test_bench$Price_NOK, test_bench$price_lag24)
metrics_lag_168 <- accuracy(test_bench$Price_NOK, test_bench$price_lag168)

```

```{r}
#Creating the table for the metrics of all the models
model <- c("sarimax", "naive1", "snaive24", "snaive168")

RMSE <- c(as.numeric(sarimax_results$RMSE_sarimax), as.numeric(metrics_lag_1[,"RMSE"]), as.numeric(metrics_lag_24[,"RMSE"]), as.numeric(metrics_lag_168[,"RMSE"]))

MAE <- c(as.numeric(sarimax_results$RMSE_sarimax), as.numeric(metrics_lag_1[,"MAE"]), as.numeric(metrics_lag_24[,"MAE"]), as.numeric(metrics_lag_168[,"MAE"]))

Error_sarimax <- sarimax_results$errors

write.csv(error_sarimax,"output/tables/error_sarimax.csv")

metrics_table <- data.frame (model, RMSE, MAE )

#saving the table as png for word
# Source - https://stackoverflow.com/a
# Posted by Ning
# Retrieved 2025-11-17, License - CC BY-SA 3.0
library(gridExtra)
png("output/tables/metrics_table.png")
grid.table(metrics_table)
dev.off()

#saving the table as txt for coding
write.csv(metrics_table, "output/tables/metrics_table.csv")

#Diebold-mariano test

#Creating the errors for e1 and e2 in diebold test
error_lag1 <- test_bench$error_naive_1

error_lag24 <- test_bench$error_naive_24

error_lag168 <- test_bench$error_naive_168

error_sarimax <- sarimax_results$errors

#diebold mariano test sarimax vs naive 1

dm_test1_P2 <- dm.test(e1 = error_sarimax, e2 = error_lag1, alternative = "less", h = 1, power = 2)
#P = 1
dm_test1_P1 <- dm.test(e1 = error_sarimax, e2 = error_lag1, alternative = "less", h = 1, power = 1)

#sarimax vs snaive 24
dm_test24_P2 <- dm.test(e1 = error_sarimax, e2 = error_lag24, alternative = "less", h = 1, power = 2)
#P = 1
dm_test24_P1 <- dm.test(e1 = error_sarimax, e2 = error_lag24, alternative = "less", h = 1, power = 1)

#sarimax vs snaive 168
dm_test168_P2 <- dm.test(e1 = error_sarimax, e2 = error_lag168, alternative = "less", h = 1, power = 2)
#P = 1
dm_test168_P1 <- dm.test(e1 = error_sarimax, e2 = error_lag168, alternative = "less", h = 1, power = 1)

Diebold_Mariano_Test <- data.frame(
  naive_1_P2TEST = dm_test1_P2$p.value,
  naive_1_P1TEST = dm_test1_P1$p.value,
  snavive_24_P2TEST = dm_test24_P2$p.value,
  snaive_24_P1TEST = dm_test24_P1$p.value,
  snaive_168_P2TEST = dm_test168_P2$p.value,
  snaive_168_P1TEST = dm_test168_P1$p.value
)
#saving the table as png for word
# Source - https://stackoverflow.com/a
# Posted by Ning
# Retrieved 2025-11-17, License - CC BY-SA 3.0
library(gridExtra)
png("output/tables/Diebold_Mariano_Test.png", width = 1000)
grid.table(Diebold_Mariano_Test)
dev.off()

#saving the table as txt for coding
write.csv(Diebold_Mariano_Test, "output/tables/Diebold_Mariano_Test.csv")
```

```{r}

forecastPlots <- data.frame(
  time = test$time,
  forecast = sarimax_results$rolling_forecasts,
  actual =  test$Price_NOK,
  error = sarimax_results$errors
)

series <- c(Actual = "blue", Forecast = "red")

forecast_actual <-ggplot(forecastPlots, aes( x = time)) +
  geom_line(aes( y = forecast, color = "Forecast"), alpha = 0.8, linewidth = 0.7)+
  geom_line(aes( y = actual, color = "Actual"), alpha = 0.8, linewidth = 0.7)+
  ylim (0,2000)+
  labs(
    title = "Actual vs. forecast",
    x = "time",
    y = "price (NOK/Mwh)"
    ) + 
  scale_color_manual("Time series", values = series)+
  theme_classic()

# since the full plot is cramped up i decided to split into 3 different plots so that it gives better information

plot1 <- forecastPlots %>% 
  filter(between(time, as.Date('2025-08-28'), as.Date('2025-09-15')))
##from sep.28 to okt.24
plot2 <- forecastPlots %>% 
  filter(between(time, as.Date('2025-09-16'), as.Date('2025-10-04')))
#plot 3
plot3 <- forecastPlots %>% 
  filter(between(time, as.Date('2025-10-5'), as.Date('2025-10-24')))

#plot 1

series <- c(Actual = "blue", Forecast = "red")

plot_1 <- ggplot(plot1, aes( x = time)) +
  geom_line(aes( y = forecast, color = "Forecast"), alpha = 0.8, linewidth = 0.7)+
  geom_line(aes( y = actual, color = "Actual"), alpha = 0.8, linewidth = 0.7)+
  ylim (0,2000)+
  labs(
    title = "Actual vs. forecast",
    x = "time",
    y = "price (NOK/Mwh)"
    ) + 
  scale_color_manual("Time series", values = series)+
  theme_classic()

##plot 2

series <- c(Actual = "blue", Forecast = "red")

plot_2 <- ggplot(plot2, aes( x = time)) +
  geom_line(aes( y = forecast, color = "Forecast"), alpha = 0.8, linewidth = 0.7)+
  geom_line(aes( y = actual, color = "Actual"), alpha = 0.8, linewidth = 0.7)+
  ylim (0,2000)+
  labs(
    title = "Actual vs. forecast",
    x = "time",
    y = "price (NOK/Mwh)"
    ) + 
  scale_color_manual("Time series", values = series)+
  theme_classic()
##plot 3
series <- c(Actual = "blue", Forecast = "red")

plot_3 <- ggplot(plot3, aes( x = time)) +
  geom_line(aes( y = forecast, color = "Forecast"), alpha = 0.8, linewidth = 0.7)+
  geom_line(aes( y = actual, color = "Actual"), alpha = 0.8, linewidth = 0.7)+
  ylim (0,2000)+
  labs(
    title = "Actual vs. forecast",
    x = "time",
    y = "price (NOK/Mwh)"
    ) + 
  scale_color_manual("Time series", values = series)+
  theme_classic()

#histogram plot of the errors using density
error_density <-ggplot(forecastPlots, aes(x = error)) +
  geom_histogram(aes(y = ..density..), binwidth = 20)+
  labs(
    title = "Forecast errors",
    y = "Density",
    x = "Errors"
  )
ggsave(filename = "output/figures/plot1.png", plot = plot_1)
ggsave(filename = "output/figures/plot2.png", plot = plot_2)
ggsave(filename = "output/figures/plot3.png", plot = plot_3)
ggsave(filename = "output/figures/forecast_actual.png", plot = forecast_actual)
ggsave(filename = "output/figures/error_density.png", plot = error_density)

```

```{r}
#To check which variables increase forecasting accuracy we take simple model and add variables

#simple model

final_data <- clean_data_dummies %>%
  mutate(y_next = lead(Price_NOK, 24)) %>%
  drop_na()

#build y and x

y<- ts(final_data$y_next, frequency = 24)

struktur_xreg <- y_next ~ HDH + demand + dummy_hour 

x <- model.matrix(struktur_xreg, data = final_data)

sarimax_simple <- forecast::Arima(
  y, 
  order = c(1,0,1), 
  seasonal = list(order = c(1,0,1), period = 24),
  xreg = x, include.mean = FALSE
)
forecast::checkresiduals(sarimax_simple)

summary(sarimax_simple)

#acf with new model#

acf(residuals(sarimax_simple), lag.max = 168)

#saving the acf plot
# Source - https://stackoverflow.com/a
# Posted by Andrie, modified by community. See post 'Timeline' for change history
# Retrieved 2025-11-17, License - CC BY-SA 3.0

png(filename="output/figures/sarimax_simple.png")
plot(acf(residuals(sarimax_simple), lag.max = 168))
dev.off()

#saving the summary
# Source - https://stackoverflow.com/a
# Posted by Phil, modified by community. See post 'Timeline' for change history
# Retrieved 2025-11-17, License - CC BY-SA 4.0

sink("output/tables/sarimax_simple.txt")
print(summary(sarimax_simple))
sink()  # returns output to the console


###simple model + energy#######

final_data <- clean_data_dummies %>%
  mutate(y_next = lead(Price_NOK, 24)) %>%
  drop_na()

#build y and x 

y<- ts(final_data$y_next, frequency = 24)

struktur_xreg <- y_next ~ HDH + demand + dummy_hour + Energy

x <- model.matrix(struktur_xreg, data = final_data)

sarimax_simple_energy <- forecast::Arima(
  y, 
  order = c(1,0,1), 
  seasonal = list(order = c(1,0,1), period = 24),
  xreg = x, include.mean = FALSE
)
forecast::checkresiduals(sarimax_simple_energy)

summary(sarimax_simple_energy)

#acf with new model#

acf(residuals(sarimax_simple_energy), lag.max = 168)

#saving the acf plot
# Source - https://stackoverflow.com/a
# Posted by Andrie, modified by community. See post 'Timeline' for change history
# Retrieved 2025-11-17, License - CC BY-SA 3.0

png(filename="output/figures/sarimax_simple_energy.png")
plot(acf(residuals(sarimax_simple_energy), lag.max = 168))
dev.off()


#saving the summary
# Source - https://stackoverflow.com/a
# Posted by Phil, modified by community. See post 'Timeline' for change history
# Retrieved 2025-11-17, License - CC BY-SA 4.0

sink("output/tables/sarimax_simple_energy.txt")
print(summary(sarimax_simple_energy))
sink()  # returns output to the console



```
